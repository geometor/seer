import ast
import contextlib
import io
import multiprocessing
from typing import Dict, List, Any # Added List, Any

import numpy as np

from geometor.seer.trials.code_trial import CodeTrial
from geometor.seer.trials.task_pair_trial import TaskPairTrial
from geometor.seer.tasks.grid import Grid


class StepCodeTrials:
    """
    Manages a collection of CodeTrial instances for a TaskStep.
    """

    def __init__(self, task_step):
        self.task_step = task_step  # parent
        self.code_trials: Dict[str, CodeTrial] = {}

    def run_trials(self):
        task = self.task_step.session_task.task
        for code_filename, code in self.task_step.get_python.items():
            code_trial = CodeTrial(self.task_step, code_filename, code, task)
            self.code_trials[code_filename] = code_trial

    def get_code_trial(self, code_filename: str) -> CodeTrial | None:
        return self.code_trials.get(code_filename)

    def get_first_code_trial(self) -> CodeTrial | None:
        """Retrieves the first CodeTrial, if any."""
        if self.code_trials:
            return next(iter(self.code_trials.values()))
        return None

    @property
    def any_train_passed(self) -> bool | None:
        """Checks if any train trials passed."""
        if not self.code_trials:
            return None  # No CodeTrials, return None
        for trial in self.code_trials.values():
            if trial.train_passed is True:
                return True  # Found at least one True
        return False  # No True found, but CodeTrials exist

    @property
    def any_test_passed(self) -> bool | None:
        """Checks if any test trials passed."""
        if not self.code_trials:
            return None  # No CodeTrials, return None
        for trial in self.code_trials.values():
            if trial.test_passed is True:
                return True  # Found at least one True
        return False  # No True found, but CodeTrials exist

    @property
    def count_trials(self) -> int:
        """Checks if any test trials passed."""
        return len(self.code_trials)

    def get_all_trials(self):
        """Returns a list of all CodeTrial objects."""
        return list(self.code_trials.values())


    def execute_trials(self, task):
        """Executes trials for all available code."""

            #  code_trial.execute_and_save_results()

    def get_best_trial(self):
        """Returns the CodeTrial with the lowest total score."""
        best_trial = None
        best_score = float('inf')  # Initialize with a high score

        for trial in self.code_trials.values():
            # Use total_score and handle None values
            if trial.total_score is not None and trial.total_score < best_score:
                best_score = trial.total_score
                best_trial = trial
        return best_trial

    @property
    def best_score(self):
        best_trial = self.get_best_trial()
        # Return total_score of the best trial
        return best_trial.total_score if best_trial else None


    @staticmethod
    def analyze_trial_data(code_trial_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyzes a list of CodeTrial data (dictionaries) and returns aggregated metrics.

        This method contains the core logic for calculating best scores, pass status,
        and extracting metrics, intended to be used by both TaskStep.summarize
        and rebuild_step_summary.

        Args:
            code_trial_data_list: A list where each item is a dictionary representing
                                  the data of a CodeTrial (e.g., loaded from JSON
                                  or generated by CodeTrial.to_dict()).

        Returns:
            A dictionary containing the analysis results:
            - best_score: The lowest score among all trials (float or None).
            - any_train_passed: True if any trial passed all its train pairs, else False/None.
            - any_test_passed: True if any trial passed all its test pairs, else False/None.
            - best_trial_metrics: Dict containing detailed metrics from the best trial's train pairs.
            - all_train_results_summary: Dict summarizing all train TaskPairTrials across all CodeTrials.
            - all_test_results_summary: Dict summarizing all test TaskPairTrials across all CodeTrials.
        """
        # Define keys used within the trial data dictionaries
        # (Should match CodeTrial.to_dict() and TaskPairTrial.to_dict())
        SCORE_KEY = "total_score" # From CodeTrial dict
        TRAIN_RESULTS_KEY = "train" # Top-level key in CodeTrial dict
        TEST_RESULTS_KEY = "test"   # Top-level key in CodeTrial dict
        TRIALS_LIST_KEY = "trials" # Key within train/test results dicts
        ERROR_KEY = "error" # Key within train/test results dicts and TaskPairTrial dict

        # Keys within TaskPairTrial dict
        MATCH_KEY = "match"
        SIZE_CORRECT_KEY = "size_correct"
        PALETTE_CORRECT_KEY = "color_palette_correct"
        COLOR_COUNT_CORRECT_KEY = "color_count_correct"
        PIXELS_OFF_KEY = "pixels_off"
        PERCENT_CORRECT_KEY = "percent_correct"

        results = {
            "best_score": None,
            "any_train_passed": None, # Start as None, becomes False if trials exist but none passed
            "any_test_passed": None,  # Start as None
            "best_trial_metrics": { # For direct inclusion in step summary
                SIZE_CORRECT_KEY: None,
                PALETTE_CORRECT_KEY: None,
                COLOR_COUNT_CORRECT_KEY: None,
                PIXELS_OFF_KEY: None,
                PERCENT_CORRECT_KEY: None,
            },
            "all_train_results_summary": {"total": 0, "passed": 0, "failed": 0},
            "all_test_results_summary": {"total": 0, "passed": 0, "failed": 0},
        }

        if not code_trial_data_list:
            # If no trials, set passed status to False explicitly and return defaults
            results["any_train_passed"] = False # Return False if no trials found
            results["any_test_passed"] = False # Return False if no trials found
            return results

        best_score = float('inf')
        found_valid_score = False
        all_train_pair_trials_data = []
        all_test_pair_trials_data = []
        best_code_trial_data = None # Store the data dict of the best CodeTrial

        # --- Pass 1: Iterate through CodeTrial data ---
        for ct_data in code_trial_data_list:
            score = ct_data.get(SCORE_KEY)
            train_results_data = ct_data.get(TRAIN_RESULTS_KEY, {})
            test_results_data = ct_data.get(TEST_RESULTS_KEY, {})

            train_pair_trials = train_results_data.get(TRIALS_LIST_KEY, [])
            test_pair_trials = test_results_data.get(TRIALS_LIST_KEY, [])

            # Aggregate TaskPairTrial data (represented as dicts)
            all_train_pair_trials_data.extend(train_pair_trials)
            all_test_pair_trials_data.extend(test_pair_trials)

            # Update best score and track the best trial data
            if score is not None and score < best_score:
                best_score = score
                found_valid_score = True
                best_code_trial_data = ct_data # Keep track of the best one

            # Check if this CodeTrial passed train/test
            # A CodeTrial passes if its execution didn't error AND all its TaskPairTrials match
            train_error = train_results_data.get(ERROR_KEY)
            test_error = test_results_data.get(ERROR_KEY)

            # Check train pass status for this CodeTrial
            if not train_error and train_pair_trials:
                # Check if all individual pairs passed (match=True and no error)
                if all(tpt.get(MATCH_KEY, False) and not tpt.get(ERROR_KEY) for tpt in train_pair_trials):
                    results["any_train_passed"] = True # Set to True if any CodeTrial passes

            # Check test pass status for this CodeTrial
            if not test_error and test_pair_trials:
                # Check if all individual pairs passed (match=True and no error)
                if all(tpt.get(MATCH_KEY, False) and not tpt.get(ERROR_KEY) for tpt in test_pair_trials):
                    results["any_test_passed"] = True # Set to True if any CodeTrial passes


        # --- Post Pass 1: Finalize results ---
        results["best_score"] = best_score if found_valid_score else None

        # If passed status is still None after checking all trials, set to False
        if results["any_train_passed"] is None:
             results["any_train_passed"] = False
        if results["any_test_passed"] is None:
             results["any_test_passed"] = False


        # --- Pass 2: Calculate overall TaskPairTrial summaries ---
        if all_train_pair_trials_data:
            passed = sum(1 for t in all_train_pair_trials_data if t.get(MATCH_KEY, False) and not t.get(ERROR_KEY))
            results["all_train_results_summary"] = {
                "total": len(all_train_pair_trials_data),
                "passed": passed,
                "failed": len(all_train_pair_trials_data) - passed,
            }
        if all_test_pair_trials_data:
            passed = sum(1 for t in all_test_pair_trials_data if t.get(MATCH_KEY, False) and not t.get(ERROR_KEY))
            results["all_test_results_summary"] = {
                "total": len(all_test_pair_trials_data),
                "passed": passed,
                "failed": len(all_test_pair_trials_data) - passed,
            }

        # --- Pass 3: Extract metrics from the best CodeTrial's *train* results ---
        if best_code_trial_data:
            best_train_trials = best_code_trial_data.get(TRAIN_RESULTS_KEY, {}).get(TRIALS_LIST_KEY, [])
            if best_train_trials:
                # Extract lists of metric values, handling potential None/missing keys
                size_correct_list = [t.get(SIZE_CORRECT_KEY) for t in best_train_trials]
                palette_correct_list = [t.get(PALETTE_CORRECT_KEY) for t in best_train_trials]
                color_count_correct_list = [t.get(COLOR_COUNT_CORRECT_KEY) for t in best_train_trials]
                pixels_off_list = [t.get(PIXELS_OFF_KEY) for t in best_train_trials if t.get(PIXELS_OFF_KEY) is not None]
                percent_correct_list = [t.get(PERCENT_CORRECT_KEY) for t in best_train_trials if t.get(PERCENT_CORRECT_KEY) is not None]

                # Calculate final metrics for the summary
                # Use all() for boolean checks, ensuring we handle None values correctly
                results["best_trial_metrics"][SIZE_CORRECT_KEY] = all(s is True for s in size_correct_list) if any(s is not None for s in size_correct_list) else None
                results["best_trial_metrics"][PALETTE_CORRECT_KEY] = all(p is True for p in palette_correct_list) if any(p is not None for p in palette_correct_list) else None
                results["best_trial_metrics"][COLOR_COUNT_CORRECT_KEY] = all(c is True for c in color_count_correct_list) if any(c is not None for c in color_count_correct_list) else None
                results["best_trial_metrics"][PIXELS_OFF_KEY] = sum(pixels_off_list) if pixels_off_list else None
                results["best_trial_metrics"][PERCENT_CORRECT_KEY] = sum(percent_correct_list) / len(percent_correct_list) if percent_correct_list else None

        return results
